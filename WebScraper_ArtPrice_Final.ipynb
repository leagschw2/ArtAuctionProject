{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping Script for ArtPrice\n",
    "# Combining Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted cookies.\n",
      "Logged in successfully\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize & Login\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")  # Optional for background mode\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Login once\n",
    "email = \"artauctionproject57@gmail.com\"\n",
    "password = \"Artauctionproject2025!\"\n",
    "logged_in = login_to_artprice(driver, email, password)\n",
    "\n",
    "if not logged_in:\n",
    "    print(\"Login failed. Exiting.\")\n",
    "    driver.quit()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping letter: A\n",
      "Starting https://www.artprice.com/artists/all/A\n",
      "40 links found for A\n"
     ]
    }
   ],
   "source": [
    "# 2. Scrape all artist URLs (A–Z)\n",
    "all_artist_links = []\n",
    "for letter in [\"A\"]: # string.ascii_uppercase\n",
    "    print(f\"Scraping letter: {letter}\")\n",
    "    try:\n",
    "        links = scrape_artist_links(driver, letter)\n",
    "        links = links[20:60] # Limit to 60 artists per letter\n",
    "        all_artist_links.extend(links)\n",
    "        print(f\"{len(links)} links found for {letter}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {letter}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking artist 1/40\n",
      "Checking artist 2/40\n",
      "Checking artist 3/40\n",
      "Checking artist 4/40\n",
      "Checking artist 5/40\n",
      "Carl Frederik AAGAARD has 440 works\n",
      "Checking artist 6/40\n",
      "Checking artist 7/40\n",
      "Martin AAGAARD has 80 works\n",
      "Checking artist 8/40\n",
      "Carl Frederik AAGAARD has 440 works\n",
      "Checking artist 9/40\n",
      "Checking artist 10/40\n",
      "Checking artist 11/40\n",
      "Martin AAGAARD has 80 works\n",
      "Checking artist 12/40\n",
      "Checking artist 13/40\n",
      "Checking artist 14/40\n",
      "Gunnar AAGAARD ANDERSEN has 91 works\n",
      "Checking artist 15/40\n",
      "Checking artist 16/40\n",
      "Carl Trier AAGÅRD has 74 works\n",
      "Checking artist 17/40\n",
      "Checking artist 18/40\n",
      "Checking artist 19/40\n",
      "Checking artist 20/40\n",
      "Checking artist 21/40\n",
      "Checking artist 22/40\n",
      "Checking artist 23/40\n",
      "Checking artist 24/40\n",
      "Checking artist 25/40\n",
      "Checking artist 26/40\n",
      "Checking artist 27/40\n",
      "Ahmed Abdel AAL has 12 works\n",
      "Checking artist 28/40\n",
      "Checking artist 29/40\n",
      "Checking artist 30/40\n",
      "Checking artist 31/40\n",
      "Checking artist 32/40\n",
      "Checking artist 33/40\n",
      "Checking artist 34/40\n",
      "Checking artist 35/40\n",
      "Checking artist 36/40\n",
      "Ilmari AALTO has 125 works\n",
      "Checking artist 37/40\n",
      "Timo AALTO has 13 works\n",
      "Checking artist 38/40\n",
      "Checking artist 39/40\n",
      "Checking artist 40/40\n"
     ]
    }
   ],
   "source": [
    "# 3. Filter for artists with > 10 artworks\n",
    "qualified_artists = []\n",
    "for idx, url in enumerate(all_artist_links):\n",
    "    print(f\"Checking artist {idx+1}/{len(all_artist_links)}\")\n",
    "    try:\n",
    "        artist_info = get_artist_info_if_has_enough_works(driver, url)\n",
    "        if artist_info:\n",
    "            qualified_artists.append(artist_info)\n",
    "            print(f\"{artist_info['name']} has {artist_info['count']} works\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping due to error: {e}\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Scraping Carl Frederik AAGAARD (1/9)\n",
      "Error with Carl Frederik AAGAARD: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a531e350>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      " Scraping Martin AAGAARD (2/9)\n",
      "Error with Martin AAGAARD: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a561c460>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      " Scraping Carl Frederik AAGAARD (3/9)\n",
      "Error with Carl Frederik AAGAARD: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a531d660>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      " Scraping Martin AAGAARD (4/9)\n",
      "Error with Martin AAGAARD: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a55e7190>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      " Scraping Gunnar AAGAARD ANDERSEN (5/9)\n",
      "Error with Gunnar AAGAARD ANDERSEN: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a55e73d0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      " Scraping Carl Trier AAGÅRD (6/9)\n",
      "Error with Carl Trier AAGÅRD: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a561c820>: Failed to establish a new connection: [Errno 61] Connection refused'))\n",
      "\n",
      " Scraping Ahmed Abdel AAL (7/9)\n",
      "Error with Ahmed Abdel AAL: HTTPConnectionPool(host='localhost', port=64561): Max retries exceeded with url: /session/ddf6d999cb32b958a75aab8519be8fb4/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a55e7c40>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4. Scrape all artworks per artist\n",
    "all_artworks = []\n",
    "for idx, artist in enumerate(qualified_artists):\n",
    "    print(f\"\\n Scraping {artist['name']} ({idx+1}/{len(qualified_artists)})\")\n",
    "    artist_url = artist[\"url\"].rstrip(\"/\") + \"/lots/pasts?p=1\"\n",
    "    \n",
    "    try:\n",
    "        artwork_links = get_all_artwork_links(driver, artist_url)\n",
    "        print(f\"Found {len(artwork_links)} artworks\")\n",
    "\n",
    "        for url in artwork_links:\n",
    "            data = scrape_artwork_detail(driver, url)\n",
    "            flat_data = flatten_artwork_data([data])\n",
    "            all_artworks.extend(flat_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {artist['name']}: {e}\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Function 1: Login to ArtPrice ----\n",
    "def login_to_artprice(driver, email, password):\n",
    "    \n",
    "    driver.get(\"https://www.artprice.com\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Accept Cookies\n",
    "    try:\n",
    "        accept_cookies = WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.sln-accept-cookies\"))\n",
    "        )\n",
    "        accept_cookies.click()\n",
    "        print(\"Accepted cookies.\")\n",
    "    except Exception:\n",
    "        print(\"No cookie banner or already accepted.\")\n",
    "    time.sleep(7)\n",
    "\n",
    "    # Click login button\n",
    "    try:\n",
    "        login_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.e2e-login-btn\"))\n",
    "        )\n",
    "        login_button.click()\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't find login button:\", e)\n",
    "        return False\n",
    "    \n",
    "    time.sleep(7)\n",
    "    \n",
    "    # Fill login form\n",
    "    try:\n",
    "        login_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"login\"))\n",
    "        )\n",
    "        password_input = driver.find_element(By.ID, \"pass\")\n",
    "        \n",
    "        login_input.clear()\n",
    "        login_input.send_keys(email)\n",
    "        password_input.clear()\n",
    "        password_input.send_keys(password)\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.e2e-login-submit-btn\"))).click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        print(\"Logged in successfully\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Login failed:\", e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Function 2: Extracting All Artist Names and URLs ----\n",
    "BASE_URL = \"https://www.artprice.com\"\n",
    "\n",
    "def scrape_artist_links(driver, letter):\n",
    "    links = []\n",
    "    url = f\"{BASE_URL}/artists/all/{letter}\"\n",
    "    print(f\"Starting {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(2)\n",
    "            artists = driver.find_elements(By.CSS_SELECTOR, \"div.col-xs-6.artist a\")\n",
    "            for a in artists:\n",
    "                href = a.get_attribute(\"href\")\n",
    "                if href:\n",
    "                    links.append(href)\n",
    "\n",
    "            # Try clicking next\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_btn)\n",
    "                driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping {letter}: {e}\")\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Function 3: Filter Artists with > 10 Artworks ----\n",
    "def get_artist_info_if_has_enough_works(driver, artist_url, min_works=10):\n",
    "    driver.get(artist_url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Find the div that contains \"Find prices (X)\"\n",
    "        elem = driver.find_element(By.XPATH, \"//div[contains(text(), 'Find prices')]\")\n",
    "        match = re.search(r'Find prices\\s*\\((\\d+)\\)', elem.text)\n",
    "        if match:\n",
    "            count = int(match.group(1))\n",
    "            if count > min_works:\n",
    "                # Get artist name from the page title or h1\n",
    "                try:\n",
    "                    artist_name = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "                except NoSuchElementException:\n",
    "                    artist_name = artist_url.split(\"/\")[-1]\n",
    "                return {\"name\": artist_name, \"url\": artist_url, \"count\": count}\n",
    "    except NoSuchElementException:\n",
    "        pass  # If the div isn't found, skip this artist\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Function 4: Extracting all Artwork URLs for each Artist ----\n",
    "def get_all_artwork_links(driver, start_url):\n",
    "    all_links = []\n",
    "    driver.get(start_url)\n",
    "\n",
    "    while True:\n",
    "        # Wait for artwork grid to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"lot-container\")))\n",
    "\n",
    "        # Parse page content\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract artwork links\n",
    "        for container in soup.find_all(\"div\", class_=\"lot-container\"):\n",
    "            a_tag = container.find(\"a\", class_=\"sln_lot_show\")\n",
    "            if a_tag:\n",
    "                href = a_tag.get(\"href\")\n",
    "                full_url = \"https://www.artprice.com\" + href\n",
    "                all_links.append(full_url)\n",
    "\n",
    "        print(f\"Scraped {len(all_links)} artworks so far...\")\n",
    "\n",
    "        # Try to click the \"Next\" button\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"li.next_page a.sln-next-page\"))\n",
    "            )\n",
    "            next_button.click()\n",
    "            time.sleep(2)  # Give time for the next page to load\n",
    "        except:\n",
    "            print(\"No more pages or 'Next' button not found.\")\n",
    "            break\n",
    "\n",
    "    return all_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Function 5: Extracting all Information from a Single Artwork ----\n",
    "def scrape_artwork_detail(driver, url):\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # -- Artist --\n",
    "    artist_tag = soup.select_one(\"div.artist a.invisiblelink\")\n",
    "    artist = artist_tag.text.strip() if artist_tag else None\n",
    "\n",
    "    # -- Title --\n",
    "    title_tag = soup.select_one(\"div.lot-title h1\")\n",
    "    title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "    # -- Lot Number --\n",
    "    lot_tag = soup.find(\"div\", class_=\"block marg marg-t-5\")\n",
    "    lot_number = lot_tag.text.strip() if lot_tag else None\n",
    "\n",
    "    # -- Description (h2/h3/h4 inside .description) --\n",
    "    description_tag = soup.find(\"div\", class_=\"description\")\n",
    "    if description_tag:\n",
    "        desc_parts = [tag.text.strip() for tag in description_tag.find_all([\"h2\", \"h3\", \"h4\"])]\n",
    "        description = \" | \".join(desc_parts)\n",
    "    else:\n",
    "        description = None\n",
    "\n",
    "    # -- Prices --\n",
    "    final_price = starting_price = estimate = price_incl_premium = None\n",
    "    prices_tag = soup.find(\"div\", class_=\"prices\")\n",
    "    if prices_tag:\n",
    "        # Final hammer price\n",
    "        hammer = prices_tag.find(\"div\", class_=\"price\")\n",
    "        if hammer and \"Hammer price\" in hammer.text:\n",
    "            span = hammer.find(\"span\", attrs={\"ng-show\": re.compile(\"currency.*eur\")})\n",
    "            final_price = span.text.strip() if span else None\n",
    "\n",
    "        # Price incl. premium\n",
    "        premium = prices_tag.find(\"div\", class_=\"price-tax\")\n",
    "        if premium:\n",
    "            span = premium.find(\"span\", attrs={\"ng-show\": re.compile(\"currency.*eur\")})\n",
    "            price_incl_premium = span.text.strip() if span else None\n",
    "\n",
    "        # Starting price\n",
    "        start = prices_tag.find(\"div\", class_=\"price-start\")\n",
    "        if start:\n",
    "            span = start.find(\"span\", attrs={\"ng-show\": re.compile(\"currency.*eur\")})\n",
    "            starting_price = span.text.strip() if span else None\n",
    "\n",
    "        # Estimate\n",
    "        estimate_spans = prices_tag.select(\"div.price-estim span[ng-show*='currency']\")\n",
    "        if estimate_spans:\n",
    "            estimate = \" - \".join([s.text.strip() for s in estimate_spans if \"eur\" in s.get(\"ng-show\", \"\")])\n",
    "\n",
    "    # -- Sale Info --\n",
    "    sale_title = sale_date = auction_house = location = None\n",
    "    sale_tag = soup.find(\"div\", class_=\"sale\")\n",
    "    if sale_tag:\n",
    "        sale_title_tag = sale_tag.select_one(\"div.strong.block i\")\n",
    "        sale_title = sale_title_tag.text.strip() if sale_title_tag else None\n",
    "\n",
    "        sale_date_tag = sale_tag.select_one(\"div.block.sale-date\")\n",
    "        sale_date = sale_date_tag.text.strip() if sale_date_tag else None\n",
    "\n",
    "        auction_house_tag = sale_tag.select_one(\"div.block.strong div\")\n",
    "        auction_house = auction_house_tag.text.strip() if auction_house_tag else None\n",
    "\n",
    "        # Get last meaningful block for location\n",
    "        blocks = sale_tag.find_all(\"div\", class_=\"block\")\n",
    "        for block in reversed(blocks):\n",
    "            text = block.get_text(strip=True)\n",
    "            if text and not any(kw in text.lower() for kw in [\"sale\", \"auctioneer\", \"date\"]):\n",
    "                location = text\n",
    "                break\n",
    "\n",
    "    # -- Additional Details --\n",
    "    additional_details = {}\n",
    "    datas_tag = soup.select_one(\"div.sale div.datas\")\n",
    "    if datas_tag:\n",
    "        for block in datas_tag.find_all(\"div\", class_=\"block\"):\n",
    "            head_span = block.find(\"span\", class_=\"head\")\n",
    "            if head_span:\n",
    "                label = head_span.text.strip().rstrip(\":\")\n",
    "                value = block.get_text(separator=\" \", strip=True).replace(f\"{label}:\", \"\").strip()\n",
    "                additional_details[label] = value\n",
    "            else:\n",
    "                # Try long content (like exhibition blocks)\n",
    "                content = block.get_text(separator=\" \", strip=True)\n",
    "                if content:\n",
    "                    key = f\"Note {len(additional_details)+1}\"\n",
    "                    additional_details[key] = content\n",
    "\n",
    "    return {\n",
    "        \"URL\": url,\n",
    "        \"Artist\": artist,\n",
    "        \"Title\": title,\n",
    "        \"Lot Number\": lot_number,\n",
    "        \"Description\": description,\n",
    "        \"Final Price (EUR)\": final_price,\n",
    "        \"Price Incl. Premium (EUR)\": price_incl_premium,\n",
    "        \"Starting Price (EUR)\": starting_price,\n",
    "        \"Estimate (EUR)\": estimate,\n",
    "        \"Sale Title\": sale_title,\n",
    "        \"Sale Date\": sale_date,\n",
    "        \"Auction House\": auction_house,\n",
    "        \"Location\": location,\n",
    "        \"Additional Details\": additional_details\n",
    "    }\n",
    "\n",
    "def flatten_artwork_data(records):\n",
    "    flat_records = []\n",
    "\n",
    "    for record in records:\n",
    "        flat = record.copy()\n",
    "        details = flat.pop(\"Additional Details\", {})  # Remove nested dict and store keys\n",
    "        for key, value in details.items():\n",
    "            flat[f\"Detail - {key}\"] = value  # Prefix to avoid column name clashes\n",
    "        flat_records.append(flat)\n",
    "\n",
    "    return flat_records\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArtAuction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
